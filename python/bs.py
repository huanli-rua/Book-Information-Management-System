# ry_deepseek.py
from flask import Flask, request, jsonify
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
from scipy.sparse import csr_matrix, save_npz, load_npz
from sklearn.neighbors import NearestNeighbors
from surprise import SVD, Dataset, Reader
from tqdm import tqdm  
import joblib
import logging
import os
import time
import schedule
from threading import Thread
from waitress import serve
from flask_cors import CORS
# ====================== é…ç½® ======================
# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    'user': 'root',
    'password': '200259',
    'host': 'localhost',
    'port': '3306',
    'database': 'ry-vue'
}

# å…¨å±€è·¯å¾„
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

# ====================== æ—¥å¿—é…ç½® ======================
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger()

# ====================== æ ¸å¿ƒåŠŸèƒ½ ======================
def refresh_models():
    """å…¨é‡æ›´æ–°æ¨¡å‹å’Œæ•°æ®ï¼ˆå‘½ä»¤è¡Œè¿›åº¦ç‰ˆï¼‰"""
    global user_item_sparse, svd_model, user_to_index, book_to_index
    
    try:
        # æ•°æ®åŠ è½½
        logger.info("ğŸ“¥ åŠ è½½è¯„åˆ†æ•°æ®...")
        engine = create_engine(
            f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}?charset=utf8mb4"
        )
        ratings = pd.read_sql('SELECT userId, bookIndex, score FROM user_reviews_dataset', engine)
        
        # æ•°æ®æ¸…æ´—
        ratings = ratings.drop_duplicates(subset=['userId', 'bookIndex'], keep='last')
        ratings = ratings[(ratings['score'] >= 1) & (ratings['score'] <= 5)]
        logger.info(f"âœ… æœ‰æ•ˆæ•°æ®åŠ è½½å®Œæˆï¼š{len(ratings)}æ¡è¯„åˆ†")

        # é‡å»ºæ˜ å°„
        user_ids = ratings['userId'].unique()
        book_ids = ratings['bookIndex'].unique()
        user_to_index = {user: idx for idx, user in enumerate(user_ids)}
        book_to_index = {book: idx for idx, book in enumerate(book_ids)}

        # æ„å»ºç¨€ç–çŸ©é˜µ
        logger.info("ğŸ”¨ æ„å»ºç¨€ç–çŸ©é˜µ...")
        row_indices = ratings['userId'].map(user_to_index)
        col_indices = ratings['bookIndex'].map(book_to_index)
        user_item_sparse = csr_matrix(
            (ratings['score'], (row_indices, col_indices)),
            shape=(len(user_ids), len(book_ids)))
        save_npz(os.path.join(MODEL_DIR, 'user_item_sparse.npz'), user_item_sparse)

        # è®­ç»ƒæ¨¡å‹
        logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒSVDæ¨¡å‹...")
        reader = Reader(rating_scale=(1, 5))
        data = Dataset.load_from_df(ratings[['userId', 'bookIndex', 'score']], reader)
        trainset = data.build_full_trainset()
        
        # åˆå§‹åŒ–æ¨¡å‹
        svd_model = SVD(n_factors=50, n_epochs=20)
        
        # ä½¿ç”¨æ ‡å‡†fitæ–¹æ³•è®­ç»ƒ
        with tqdm(total=svd_model.n_epochs, desc="æ¨¡å‹è®­ç»ƒ", unit="epoch") as pbar:
            # é‡å†™è®­ç»ƒé€»è¾‘ä»¥æ”¯æŒè¿›åº¦æ¡
            for _ in range(svd_model.n_epochs):
                svd_model.fit(trainset)  # ç›´æ¥è°ƒç”¨fitä¼šå®Œæˆæ‰€æœ‰epochï¼Œéœ€æ‰‹åŠ¨æ¨¡æ‹Ÿè¿›åº¦
                pbar.update(1)
        
        joblib.dump(svd_model, os.path.join(MODEL_DIR, 'svd_model.pkl'))

        # ç”Ÿæˆæ¨èç»“æœ
        logger.info("ğŸ“¤ ç”Ÿæˆæ¨èç»“æœ...")
        total_users = len(user_ids)
        batch_size = 500
        
        with engine.begin() as conn:
            conn.execute(text("TRUNCATE TABLE recommendation"))
            
        with tqdm(total=total_users, desc="ç”¨æˆ·å¤„ç†è¿›åº¦", unit="user") as main_pbar:
            for i in range(0, total_users, batch_size):
                batch_users = user_ids[i:i+batch_size]
                recommendations = []
                
                for user_id in batch_users:
                    rated = ratings[ratings['userId'] == user_id]['bookIndex'].tolist()
                    unrated = list(set(book_ids) - set(rated))
                    
                    user_preds = []
                    for book in unrated:
                        try:
                            pred = svd_model.predict(user_id, book)
                            user_preds.append( (book, pred.est) )
                        except:
                            continue
                    top_50 = sorted(user_preds, key=lambda x: x[1], reverse=True)[:50]
                    recommendations.extend([(user_id, book, score) for book, score in top_50])
                
                if recommendations:
                    df = pd.DataFrame(recommendations, columns=['userId', 'bookIndex', 'predicted_score'])
                    df.to_sql('recommendation', engine, if_exists='append', index=False, method='multi')
                
                main_pbar.update(len(batch_users))
                main_pbar.set_postfix({"è¿›åº¦": f"{min(i+batch_size, total_users)}/{total_users}"})

        # ä¿å­˜å…ƒæ•°æ®
        np.save(os.path.join(MODEL_DIR, 'user_to_index.npy'), user_to_index)
        np.save(os.path.join(MODEL_DIR, 'book_to_index.npy'), book_to_index)
        
        logger.info("ğŸ‰ å…¨é‡æ›´æ–°å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°å¤±è´¥: {str(e)}")
        raise

def update_user_recommendations(user_id):
    """ä¸ºå•ä¸ªç”¨æˆ·æ›´æ–°æ¨èç»“æœ"""
    try:
        global svd_model, user_to_index, book_to_index
        
        # åŠ è½½æ¨¡å‹å’Œæ˜ å°„è¡¨ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
        if 'svd_model' not in globals():
            svd_model = joblib.load(os.path.join(MODEL_DIR, 'svd_model.pkl'))
            user_to_index = np.load(os.path.join(MODEL_DIR, 'user_to_index.npy'), allow_pickle=True).item()
            book_to_index = np.load(os.path.join(MODEL_DIR, 'book_to_index.npy'), allow_pickle=True).item()
        
        # è·å–ç”¨æˆ·è¯„åˆ†æ•°æ®
        engine = create_engine(
            f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}?charset=utf8mb4"
        )
        ratings = pd.read_sql(f'SELECT bookIndex, score FROM user_reviews_dataset WHERE userId={user_id}', engine)
        rated_books = ratings['bookIndex'].tolist()
        
        # è·å–æ‰€æœ‰ä¹¦ç±IDå¹¶è¿‡æ»¤æœªè¯„åˆ†çš„
        all_books = pd.read_sql('SELECT DISTINCT bookIndex FROM user_reviews_dataset', engine)['bookIndex'].tolist()
        unrated_books = list(set(all_books) - set(rated_books))
        
        # ç”Ÿæˆé¢„æµ‹è¯„åˆ†
        recommendations = []
        for book in unrated_books:
            try:
                pred = svd_model.predict(user_id, book)
                recommendations.append((user_id, book, pred.est))
            except:
                continue
        
        # å–Top50å¹¶æ›´æ–°æ•°æ®åº“
        top_50 = sorted(recommendations, key=lambda x: x[2], reverse=True)[:50]
        df = pd.DataFrame(top_50, columns=['userId', 'bookIndex', 'predicted_score'])
        
        with engine.begin() as conn:
            # åˆ é™¤æ—§æ¨è
            conn.execute(text(f"DELETE FROM recommendation WHERE userId={user_id}"))
            # æ’å…¥æ–°æ¨è
            if not df.empty:
                df.to_sql('recommendation', conn, if_exists='append', index=False, method='multi')
        
        logger.info(f"ğŸ”„ ç”¨æˆ·{user_id}æ¨èæ•°æ®å¢é‡æ›´æ–°å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ ç”¨æˆ·{user_id}å¢é‡æ›´æ–°å¤±è´¥: {str(e)}")
        raise


# ====================== Flaskæ¥å£ ======================
app = Flask(__name__)
CORS(app)  # è§£å†³è·¨åŸŸé—®é¢˜

# åœ¨ handle_rating æ¥å£ä¸­æ·»åŠ æ•°æ®åº“æ’å…¥é€»è¾‘
@app.route('/update', methods=['POST'])
def handle_rating():
    data = request.json
    try:
        user_id = int(data.get('userId'))
        book_id = int(data.get('bookIndex'))
        score = float(data.get('score'))
        comment = data.get('comment', '')
    except (TypeError, ValueError) as e:
        logger.error(f"å‚æ•°ç±»å‹é”™è¯¯: {str(e)}")
        return jsonify({"status": "error", "message": "Invalid parameter type"}), 400

    engine = create_engine(f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}?charset=utf8mb4")
    
    try:
        with engine.begin() as conn:
            conn.execute(text("""
                INSERT INTO user_reviews_dataset (userId, bookIndex, score, comment)
                VALUES (:uid, :bid, :score, :comment)
                ON DUPLICATE KEY UPDATE 
                    score = VALUES(score),
                    comment = VALUES(comment)
            """), {
                "uid": user_id,
                "bid": book_id,
                "score": score,
                "comment": comment
            })
            
        # æ–°å¢ï¼šè§¦å‘å¢é‡æ›´æ–°
        update_user_recommendations(user_id)
        
        return jsonify({"status": "success", "message": "Recommendations updated"})
        
    except Exception as e:
        logger.error(f"âŒ ç”¨æˆ·{user_id}æ›´æ–°å¤±è´¥: {str(e)}")
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/recommend/<int:user_id>')
def get_recommend(user_id):
    try:
        engine = create_engine(
            f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}?charset=utf8mb4"
        )
        is_new = pd.read_sql(
            f"SELECT COUNT(*) AS cnt FROM user_reviews_dataset WHERE userId={user_id}", 
            engine
        ).iloc[0]['cnt'] == 0
        
        if is_new:
            hot_books = pd.read_sql(
                """SELECT bookIndex FROM user_reviews_dataset 
                   GROUP BY bookIndex HAVING COUNT(*) > 10 
                   ORDER BY AVG(score) DESC LIMIT 50""", 
                engine
            )['bookIndex'].tolist()
            return jsonify({"type": "hot", "books": hot_books})
        else:
            recs = pd.read_sql(
                f"""SELECT bookIndex, predicted_score 
                    FROM recommendation 
                    WHERE userId={user_id} 
                    ORDER BY predicted_score DESC LIMIT 50""",
                engine
            ).to_dict('records')
            return jsonify({"type": "personalized", "books": recs})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

# ====================== å®šæ—¶ä»»åŠ¡ ======================
def schedule_retrain():
    while True:
        schedule.every().day.at("03:00").do(refresh_models)
        time.sleep(60)
        schedule.run_pending()

# ====================== ä¸»ç¨‹åº ======================
if __name__ == '__main__':
    # åˆå§‹åŒ–æ£€æŸ¥
    if not os.path.exists(os.path.join(MODEL_DIR, 'svd_model.pkl')):
        logger.info("â³ æœªæ£€æµ‹åˆ°æ¨¡å‹ï¼Œå¼€å§‹åˆå§‹åŒ–è®­ç»ƒ...")
        refresh_models()
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡
    Thread(target=schedule_retrain, daemon=True).start()
    
    # å¯åŠ¨æœåŠ¡
    logger.info("ğŸš€ æœåŠ¡å·²å¯åŠ¨ï¼šhttp://localhost:5000")
    serve(app, host='0.0.0.0', port=5000)